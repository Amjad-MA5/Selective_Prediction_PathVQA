{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_predictions import *\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load ViLT Model fined tuned on VQAv2 dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViltForQuestionAnswering(\n",
       "  (vilt): ViltModel(\n",
       "    (embeddings): ViltEmbeddings(\n",
       "      (text_embeddings): TextEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768)\n",
       "        (position_embeddings): Embedding(40, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (patch_embeddings): ViltPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
       "      )\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViltEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViltLayer(\n",
       "          (attention): ViltAttention(\n",
       "            (attention): ViltSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViltSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViltIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViltOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViltPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1536, bias=True)\n",
       "    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=1536, out_features=3129, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, processor = get_model()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Validation set from VQAv2 dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/Selective_Prediction_VQA/dataset/data/VQAv2c.hf\n",
      "Loading from local path /teamspace/studios/this_studio/Selective_Prediction_VQA/dataset/data/VQAv2c.hf.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5fc5a71ccc4b779cd87fbb986a2eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ee9f1f1f6e4d7b88589b237e78f4e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c8b085580246ca8ec0beefa707caa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/teamspace/studios/this_studio/Selective_Prediction_VQA\")\n",
    "from dataset.get_VQAv2_dataset import get_VQAv2_dataset\n",
    "dataset = get_VQAv2_dataset()\n",
    "validation_dataset = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data for batch no: 0\n",
      "Saved data for batch no: 1\n",
      "Saved data for batch no: 2\n",
      "Saved data for batch no: 3\n",
      "Saved data for batch no: 4\n",
      "Saved data for batch no: 5\n",
      "Saved data for batch no: 6\n",
      "Saved data for batch no: 7\n",
      "Saved data for batch no: 8\n",
      "Saved data for batch no: 9\n",
      "Saved data for batch no: 10\n",
      "Saved data for batch no: 11\n",
      "Saved data for batch no: 12\n",
      "Saved data for batch no: 13\n",
      "Saved data for batch no: 14\n",
      "Saved data for batch no: 15\n",
      "Saved data for batch no: 16\n",
      "Saved data for batch no: 17\n",
      "Saved data for batch no: 18\n",
      "Saved data for batch no: 19\n",
      "Saved data for batch no: 20\n",
      "Saved data for batch no: 21\n",
      "Saved data for batch no: 22\n",
      "Unsupported number of image dimensions: 2\n",
      "Unsupported number of image dimensions: 2\n",
      "Unsupported number of image dimensions: 2\n",
      "Unsupported number of image dimensions: 2\n",
      "Unsupported number of image dimensions: 2\n",
      "Saved data for batch no: 23\n",
      "Saved data for batch no: 24\n",
      "Saved data for batch no: 25\n",
      "Saved data for batch no: 26\n",
      "Saved data for batch no: 27\n",
      "Saved data for batch no: 28\n",
      "Saved data for batch no: 29\n",
      "Saved data for batch no: 30\n",
      "Saved data for batch no: 31\n"
     ]
    }
   ],
   "source": [
    "logits, labels = prediction(model, processor, validation_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logits and labels have been store in .pt file for calibration methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Path : /teamspace/studios/this_studio/Selective_Prediction_VQA/predictions/logits_and_labels/Logits_and_labels0.pt"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
